package gql

// This file will be automatically regenerated based on the schema, any resolver
// implementations
// will be copied through when generating and any unknown code will be moved to the end.
// Code generated by github.com/99designs/gqlgen

import (
	"context"
	"fmt"
	"sort"
	"time"

	"entgo.io/ent/dialect"
	"entgo.io/ent/dialect/sql"
	"github.com/looplj/axonhub/internal/ent"
	"github.com/looplj/axonhub/internal/ent/apikey"
	"github.com/looplj/axonhub/internal/ent/channel"
	"github.com/looplj/axonhub/internal/ent/project"
	"github.com/looplj/axonhub/internal/ent/request"
	"github.com/looplj/axonhub/internal/ent/requestexecution"
	"github.com/looplj/axonhub/internal/ent/schema/schematype"
	"github.com/looplj/axonhub/internal/ent/usagelog"
	"github.com/looplj/axonhub/internal/log"
	"github.com/looplj/axonhub/internal/objects"
	"github.com/looplj/axonhub/internal/pkg/xtime"
	"github.com/looplj/axonhub/internal/scopes"
	"github.com/samber/lo"
)

// DashboardOverview is the resolver for the dashboardOverview field.
// Note: This resolver provides high-level dashboard metrics.
// For detailed request statistics, see RequestStats resolver documentation.
func (r *queryResolver) DashboardOverview(ctx context.Context) (*DashboardOverview, error) {
	ctx = scopes.WithUserScopeDecision(ctx, scopes.ScopeReadDashboard)

	// Initialize response with defaults to handle partial failures gracefully
	stats := &DashboardOverview{
		TotalRequests:       0,
		FailedRequests:      0,
		AverageResponseTime: nil,
	}

	// Get total and failed requests in a single query by status grouping
	var statusCounts []struct {
		Status request.Status `json:"status"`
		Count  int            `json:"count"`
	}
	if err := r.client.Request.Query().
		GroupBy(request.FieldStatus).
		Aggregate(ent.Count()).
		Scan(ctx, &statusCounts); err != nil {
		log.Warn(ctx, "failed to count requests by status", log.Cause(err))
	} else {
		for _, sc := range statusCounts {
			stats.TotalRequests += sc.Count
			if sc.Status == request.StatusFailed {
				stats.FailedRequests = sc.Count
			}
		}
	}

	// Get request stats using the dedicated resolver
	if requestStats, err := r.RequestStats(ctx); err != nil {
		log.Warn(ctx, "failed to get request stats", log.Cause(err))
		// Set a default empty RequestStats if there's an error
		stats.RequestStats = &RequestStats{
			RequestsToday:     0,
			RequestsThisWeek:  0,
			RequestsLastWeek:  0,
			RequestsThisMonth: 0,
		}
	} else {
		stats.RequestStats = requestStats
	}

	// TODO: Calculate average response time from request execution data
	// This would require additional database schema changes to store response times

	return stats, nil
}

// RequestStats is the resolver for the requestStats field.
// Note: For result-only statistics (e.g., successful request counts), use the usage_logs table.
// For process tracking (e.g., failed requests), use request/request_execution tables.
// For channel-level statistics, use request_execution table.
func (r *queryResolver) RequestStats(ctx context.Context) (*RequestStats, error) {
	ctx = scopes.WithUserScopeDecision(ctx, scopes.ScopeReadDashboard)

	// Initialize response with defaults to handle partial failures gracefully
	stats := &RequestStats{
		RequestsToday:     0,
		RequestsThisWeek:  0,
		RequestsLastWeek:  0,
		RequestsThisMonth: 0,
	}

	loc := r.systemService.TimeLocation(ctx)
	period := xtime.GetCalendarPeriods(loc)

	if requestsToday, err := r.client.UsageLog.Query().
		Where(usagelog.CreatedAtGTE(period.Today.Start)).
		Count(ctx); err != nil {
		log.Warn(ctx, "failed to count today's requests", log.Cause(err))
	} else {
		stats.RequestsToday = requestsToday
	}

	if requestsThisWeek, err := r.client.UsageLog.Query().
		Where(usagelog.CreatedAtGTE(period.ThisWeek.Start)).
		Count(ctx); err != nil {
		log.Warn(ctx, "failed to count this week's requests", log.Cause(err))
	} else {
		stats.RequestsThisWeek = requestsThisWeek
	}

	if requestsLastWeek, err := r.client.UsageLog.Query().
		Where(usagelog.CreatedAtGTE(period.LastWeek.Start), usagelog.CreatedAtLT(period.LastWeek.End)).
		Count(ctx); err != nil {
		log.Warn(ctx, "failed to count last week's requests", log.Cause(err))
	} else {
		stats.RequestsLastWeek = requestsLastWeek
	}

	if requestsThisMonth, err := r.client.UsageLog.Query().
		Where(usagelog.CreatedAtGTE(period.ThisMonth.Start)).
		Count(ctx); err != nil {
		log.Warn(ctx, "failed to count this month's requests", log.Cause(err))
	} else {
		stats.RequestsThisMonth = requestsThisMonth
	}

	return stats, nil
}

// RequestStatsByChannel is the resolver for the requestStatsByChannel field.
// Note: Uses usage_logs table for result-only statistics aggregated by channel.
// For channel-level process tracking (e.g., success/failure rates), use request_execution table.
func (r *queryResolver) RequestStatsByChannel(ctx context.Context) ([]*RequestStatsByChannel, error) {
	ctx = scopes.WithUserScopeDecision(ctx, scopes.ScopeReadDashboard)

	// Use efficient aggregation query with JOIN to get channel details and filter out deleted channels
	type channelStats struct {
		ChannelName string `json:"channel_name"`
		Count       int    `json:"count"`
	}

	var results []channelStats

	// Aggregate by channel directly in the database using usage_logs table joined with channels
	err := r.client.UsageLog.Query().
		Modify(func(s *sql.Selector) {
			channelTable := sql.Table(channel.Table)
			s.Join(channelTable).On(
				s.C(usagelog.FieldChannelID),
				channelTable.C(channel.FieldID),
			)

			// Filter: only non-deleted channels
			s.Where(sql.EQ(channelTable.C(channel.FieldDeletedAt), 0))

			// Group by channel fields to get names and types directly
			s.GroupBy(channelTable.C(channel.FieldName))

			// Select fields: channel name, type and the count of logs
			s.Select(
				sql.As(channelTable.C(channel.FieldName), "channel_name"),
				sql.As(sql.Count(s.C(usagelog.FieldID)), "count"),
			)

			// Order by count descending and limit to top 10
			s.OrderBy(sql.Desc("count")).Limit(10)
		}).
		Scan(ctx, &results)

	if err != nil {
		return nil, fmt.Errorf("failed to get requests by channel: %w", err)
	}

	// Build response directly from aggregated results
	return lo.Map(results, func(item channelStats, _ int) *RequestStatsByChannel {
		return &RequestStatsByChannel{
			ChannelName: item.ChannelName,
			Count:       item.Count,
		}
	}), nil
}

// RequestStatsByModel is the resolver for the requestStatsByModel field.
// Note: Uses usage_logs table for result-only statistics aggregated by model.
// This provides successful request counts per model.
func (r *queryResolver) RequestStatsByModel(ctx context.Context) ([]*RequestStatsByModel, error) {
	ctx = scopes.WithUserScopeDecision(ctx, scopes.ScopeReadDashboard)

	type modelStats struct {
		ModelID string `json:"model_id"`
		Count   int    `json:"request_count"`
	}

	var results []modelStats

	err := r.client.UsageLog.Query().
		GroupBy(usagelog.FieldModelID).
		Aggregate(ent.As(ent.Count(), "request_count")).
		Scan(ctx, &results)
	if err != nil {
		return nil, fmt.Errorf("failed to get requests by model: %w", err)
	}

	// Order by request count and keep only top 10
	sort.Slice(results, func(i, j int) bool {
		return results[i].Count > results[j].Count
	})

	if len(results) > 10 {
		results = results[:10]
	}

	stats := lo.Map(results, func(item modelStats, _ int) *RequestStatsByModel {
		return &RequestStatsByModel{
			ModelID: item.ModelID,
			Count:   item.Count,
		}
	})

	return stats, nil
}

// RequestStatsByAPIKey is the resolver for the requestStatsByAPIKey field.
// Note: Uses usage_logs table for result-only statistics aggregated by API key.
// This provides successful request counts per API key.
func (r *queryResolver) RequestStatsByAPIKey(ctx context.Context) ([]*RequestStatsByAPIKey, error) {
	ctx = scopes.WithUserScopeDecision(ctx, scopes.ScopeReadDashboard)

	type apiKeyStats struct {
		APIKeyID int `json:"api_key_id"`
		Count    int `json:"request_count"`
	}

	var results []apiKeyStats

	// Database-level aggregation
	err := r.client.UsageLog.Query().
		Where(usagelog.APIKeyIDNotNil()).
		GroupBy(usagelog.FieldAPIKeyID).
		Aggregate(ent.As(ent.Count(), "request_count")).
		Scan(ctx, &results)
	if err != nil {
		return nil, fmt.Errorf("failed to get requests by API key: %w", err)
	}

	if len(results) == 0 {
		return []*RequestStatsByAPIKey{}, nil
	}

	// Sort by count (descending) and limit to top 10
	sort.Slice(results, func(i, j int) bool {
		return results[i].Count > results[j].Count
	})

	if len(results) > 10 {
		results = results[:10]
	}

	// Extract API key IDs
	apiKeyIDs := lo.Map(results, func(item apiKeyStats, _ int) int {
		return item.APIKeyID
	})

	// Fetch API key details
	apiKeys, err := r.client.APIKey.Query().
		Where(apikey.IDIn(apiKeyIDs...)).
		All(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to get API keys: %w", err)
	}

	// Create lookup map
	apiKeyMap := lo.SliceToMap(apiKeys, func(ak *ent.APIKey) (int, *ent.APIKey) {
		return ak.ID, ak
	})

	// Build response
	var response []*RequestStatsByAPIKey

	for _, result := range results {
		if ak, exists := apiKeyMap[result.APIKeyID]; exists {
			response = append(response, &RequestStatsByAPIKey{
				APIKeyID:   objects.GUID{Type: "APIKey", ID: result.APIKeyID},
				APIKeyName: ak.Name,
				Count:      result.Count,
			})
		}
	}

	return response, nil
}

// TokenStatsByAPIKey is the resolver for the tokenStatsByAPIKey field.
// Note: Uses usage_logs table for token consumption statistics aggregated by API key.
// This provides actual token usage (input, output, cached, reasoning) per API key.
func (r *queryResolver) TokenStatsByAPIKey(ctx context.Context) ([]*TokenStatsByAPIKey, error) {
	ctx = scopes.WithUserScopeDecision(ctx, scopes.ScopeReadDashboard)

	type tokenStats struct {
		APIKeyID        int   `json:"api_key_id"`
		InputTokens     int64 `json:"input_tokens"`
		OutputTokens    int64 `json:"output_tokens"`
		CachedTokens    int64 `json:"cached_tokens"`
		ReasoningTokens int64 `json:"reasoning_tokens"`
	}

	var results []tokenStats

	// Database-level aggregation with JOIN
	err := r.client.UsageLog.Query().
		Modify(func(s *sql.Selector) {
			// Join to requests table to get api_key_id
			requestTable := sql.Table(request.Table)
			s.Join(requestTable).On(
				s.C(usagelog.FieldRequestID),
				requestTable.C(request.FieldID),
			)

			// Filter: only requests with non-null api_key_id
			s.Where(sql.NotNull(requestTable.C(request.FieldAPIKeyID)))

			// Group by api_key_id
			s.GroupBy(requestTable.C(request.FieldAPIKeyID))

			// Select aggregations
			s.Select(
				sql.As(requestTable.C(request.FieldAPIKeyID), "api_key_id"),
				sql.As(sql.Sum(s.C(usagelog.FieldPromptTokens)), "input_tokens"),
				sql.As(sql.Sum(s.C(usagelog.FieldCompletionTokens)), "output_tokens"),
				sql.As(fmt.Sprintf("COALESCE(SUM(%s), 0)", s.C(usagelog.FieldPromptCachedTokens)), "cached_tokens"),
				sql.As(fmt.Sprintf("COALESCE(SUM(%s), 0)", s.C(usagelog.FieldCompletionReasoningTokens)), "reasoning_tokens"),
			)
		}).
		Scan(ctx, &results)
	if err != nil {
		return nil, fmt.Errorf("failed to get tokens by API key: %w", err)
	}

	if len(results) == 0 {
		return []*TokenStatsByAPIKey{}, nil
	}

	// Sort by total tokens (descending) and limit to top 3
	sort.Slice(results, func(i, j int) bool {
		totalI := results[i].InputTokens + results[i].OutputTokens +
			results[i].CachedTokens + results[i].ReasoningTokens
		totalJ := results[j].InputTokens + results[j].OutputTokens +
			results[j].CachedTokens + results[j].ReasoningTokens

		return totalI > totalJ
	})

	if len(results) > 3 {
		results = results[:3]
	}

	// Extract API key IDs
	apiKeyIDs := lo.Map(results, func(item tokenStats, _ int) int {
		return item.APIKeyID
	})

	// Fetch API key details
	apiKeys, err := r.client.APIKey.Query().
		Where(apikey.IDIn(apiKeyIDs...)).
		All(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to get API keys: %w", err)
	}

	// Create lookup map
	apiKeyMap := lo.SliceToMap(apiKeys, func(ak *ent.APIKey) (int, *ent.APIKey) {
		return ak.ID, ak
	})

	// Build response
	var response []*TokenStatsByAPIKey

	for _, result := range results {
		if ak, exists := apiKeyMap[result.APIKeyID]; exists {
			totalTokens := result.InputTokens + result.OutputTokens +
				result.CachedTokens + result.ReasoningTokens

			response = append(response, &TokenStatsByAPIKey{
				APIKeyID:        objects.GUID{Type: "APIKey", ID: result.APIKeyID},
				APIKeyName:      ak.Name,
				InputTokens:     int(result.InputTokens),
				OutputTokens:    int(result.OutputTokens),
				CachedTokens:    int(result.CachedTokens),
				ReasoningTokens: int(result.ReasoningTokens),
				TotalTokens:     int(totalTokens),
			})
		}
	}

	return response, nil
}

// DailyRequestStats is the resolver for the dailyRequestStats field.
// Note: Uses usage_logs table for daily aggregated statistics (count, tokens, cost).
// Provides result-only daily metrics for the last 30 days.
func (r *queryResolver) DailyRequestStats(ctx context.Context) ([]*DailyRequestStats, error) {
	ctx = scopes.WithUserScopeDecision(ctx, scopes.ScopeReadDashboard)

	daysCount := 30

	loc := r.systemService.TimeLocation(ctx)
	nowLocal := xtime.UTCNow().In(loc)
	startDateLocal := time.Date(nowLocal.Year(), nowLocal.Month(), nowLocal.Day(), 0, 0, 0, 0, loc).AddDate(0, 0, -daysCount+1)
	startDateUTC := startDateLocal.UTC()
	endDateUTC := startDateLocal.AddDate(0, 0, daysCount).UTC()
	_, offsetSeconds := nowLocal.Zone()

	// Use GROUP BY aggregation for efficient database-level computation
	type dailyStats struct {
		Date   string  `json:"date"`
		Count  int     `json:"total_count"`
		Tokens int     `json:"total_tokens"`
		Cost   float64 `json:"total_cost"`
	}

	var results []dailyStats

	// Use raw SQL for complex GROUP BY with conditional counting
	err := r.client.UsageLog.Query().
		Where(
			usagelog.CreatedAtGTE(startDateUTC),
			usagelog.CreatedAtLT(endDateUTC),
		).
		Modify(func(s *sql.Selector) {
			// Build a dialect-specific date expression that returns a string 'YYYY-MM-DD'
			var dateExpr string
			// Use qualified column name to avoid ambiguity when joining
			createdAtCol := s.C(usagelog.FieldCreatedAt)

			switch s.Dialect() {
			case dialect.SQLite:
				dateExpr = fmt.Sprintf("strftime('%%Y-%%m-%%d', datetime(substr(%s, 1, 19), '%+d seconds'))", createdAtCol, offsetSeconds)
			case dialect.MySQL:
				dateExpr = fmt.Sprintf("DATE_FORMAT(CONVERT_TZ(%s, '+00:00', '%s'), '%%Y-%%m-%%d')", createdAtCol, loc.String())
			case dialect.Postgres:
				dateExpr = fmt.Sprintf("to_char(%s AT TIME ZONE '%s', 'YYYY-MM-DD')", createdAtCol, loc.String())
			default:
				// Fallback to ANSI-ish cast; many DBs accept this, but not guaranteed
				dateExpr = fmt.Sprintf("DATE(%s)", createdAtCol)
			}

			s.Select(
				sql.As(dateExpr, "date"),
				sql.As(sql.Count(s.C(usagelog.FieldID)), "total_count"),
				sql.As(sql.Sum(s.C(usagelog.FieldTotalTokens)), "total_tokens"),
				sql.As(fmt.Sprintf("COALESCE(SUM(%s), 0)", s.C(usagelog.FieldTotalCost)), "total_cost"),
			).
				GroupBy(dateExpr).
				OrderBy("date")
		}).
		Scan(ctx, &results)
	if err != nil {
		return nil, fmt.Errorf("failed to get daily request stats: %w", err)
	}

	// Create a map for fast lookup of aggregated data
	statsMap := lo.SliceToMap(results, func(item dailyStats) (string, dailyStats) {
		return item.Date, item
	})

	// Build complete response with zero values for missing dates
	response := make([]*DailyRequestStats, 0, daysCount)

	for i := range daysCount {
		date := startDateLocal.AddDate(0, 0, i)
		dateStr := date.Format("2006-01-02")

		if stats, exists := statsMap[dateStr]; exists {
			// Use aggregated data from database
			response = append(response, &DailyRequestStats{
				Date:   dateStr,
				Count:  stats.Count,
				Tokens: stats.Tokens,
				Cost:   stats.Cost,
			})
		} else {
			// Fill missing dates with zero values
			response = append(response, &DailyRequestStats{
				Date:   dateStr,
				Count:  0,
				Tokens: 0,
				Cost:   0,
			})
		}
	}

	return response, nil
}

// TopRequestsProjects is the resolver for the topRequestsProjects field.
// Note: Uses usage_logs table for project-level request statistics.
// Provides result-only request counts per project.
func (r *queryResolver) TopRequestsProjects(ctx context.Context) ([]*TopRequestsProjects, error) {
	ctx = scopes.WithUserScopeDecision(ctx, scopes.ScopeReadDashboard)

	limitCount := 10

	type projectRequestCount struct {
		ProjectID    int `json:"project_id"`
		RequestCount int `json:"request_count"`
	}

	var results []projectRequestCount

	// Use database aggregation without ordering (GroupBy doesn't support Order)
	err := r.client.UsageLog.Query().
		Limit(limitCount).
		Modify(func(s *sql.Selector) {
			s.Select(
				usagelog.FieldProjectID,
				sql.As(sql.Count("*"), "request_count"),
			).
				GroupBy(usagelog.FieldProjectID).
				OrderBy(sql.Desc("request_count"))
		}).
		Scan(ctx, &results)
	if err != nil {
		return nil, fmt.Errorf("failed to get top projects: %w", err)
	}

	if len(results) == 0 {
		return []*TopRequestsProjects{}, nil
	}

	// Get project details for the top projects
	projectIDs := lo.Map(results, func(item projectRequestCount, _ int) int {
		return item.ProjectID
	})

	projects, err := r.client.Project.Query().
		Where(project.IDIn(projectIDs...)).
		All(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to get project details: %w", err)
	}

	projectMap := lo.SliceToMap(projects, func(p *ent.Project) (int, *ent.Project) {
		return p.ID, p
	})

	// Build response with project details
	var response []*TopRequestsProjects

	for _, result := range results {
		if p, exists := projectMap[result.ProjectID]; exists {
			response = append(response, &TopRequestsProjects{
				ProjectID:          objects.GUID{Type: "Project", ID: p.ID},
				ProjectName:        p.Name,
				ProjectDescription: p.Description,
				RequestCount:       result.RequestCount,
			})
		}
	}

	return response, nil
}

// TokenStats is the resolver for the tokenStats field.
// Note: Uses usage_logs table for token consumption statistics (today, this week, this month).
// Provides result-only token metrics aggregated by calendar periods.
func (r *queryResolver) TokenStats(ctx context.Context) (*TokenStats, error) {
	ctx = scopes.WithUserScopeDecision(ctx, scopes.ScopeReadDashboard)

	// Initialize response with defaults to handle partial failures gracefully
	stats := &TokenStats{
		TotalInputTokensToday:      0,
		TotalOutputTokensToday:     0,
		TotalCachedTokensToday:     0,
		TotalInputTokensThisWeek:   0,
		TotalOutputTokensThisWeek:  0,
		TotalCachedTokensThisWeek:  0,
		TotalInputTokensThisMonth:  0,
		TotalOutputTokensThisMonth: 0,
		TotalCachedTokensThisMonth: 0,
	}

	loc := r.systemService.TimeLocation(ctx)
	period := xtime.GetCalendarPeriods(loc)

	// Helper function to get token sums for a specific time period
	getTokenSums := func(since time.Time) (input, output, cached int) {
		type tokenSums struct {
			InputTokens  int `json:"input_tokens"`
			OutputTokens int `json:"output_tokens"`
			CachedTokens int `json:"cached_tokens"`
		}

		var records []tokenSums

		err := r.client.UsageLog.Query().
			Where(usagelog.CreatedAtGTE(since)).
			Modify(func(s *sql.Selector) {
				s.Select(
					sql.As(sql.Sum(usagelog.FieldPromptTokens), "input_tokens"),
					sql.As(sql.Sum(usagelog.FieldCompletionTokens), "output_tokens"),
					sql.As(fmt.Sprintf("COALESCE(SUM(%s), 0)", s.C(usagelog.FieldPromptCachedTokens)), "cached_tokens"),
				)
			}).
			Scan(ctx, &records)
		if err != nil || len(records) == 0 {
			log.Warn(ctx, "failed to aggregate token stats", log.Cause(err))
			return 0, 0, 0
		}

		inputVal := records[0].InputTokens
		outputVal := records[0].OutputTokens
		cachedVal := records[0].CachedTokens

		if log.DebugEnabled(ctx) {
			log.Debug(ctx, "token stats query result",
				log.String("since", since.Format("2006-01-02 15:04:05")),
				log.Int("input", inputVal),
				log.Int("output", outputVal),
				log.Int("cached", cachedVal))
		}

		return inputVal, outputVal, cachedVal
	}

	// Get token stats for today
	input, output, cached := getTokenSums(period.Today.Start)
	stats.TotalInputTokensToday = input
	stats.TotalOutputTokensToday = output
	stats.TotalCachedTokensToday = cached

	// Get token stats for this week (calendar week from Monday)
	input, output, cached = getTokenSums(period.ThisWeek.Start)
	stats.TotalInputTokensThisWeek = input
	stats.TotalOutputTokensThisWeek = output
	stats.TotalCachedTokensThisWeek = cached

	// Get token stats for this month (calendar month from 1st)
	input, output, cached = getTokenSums(period.ThisMonth.Start)
	stats.TotalInputTokensThisMonth = input
	stats.TotalOutputTokensThisMonth = output
	stats.TotalCachedTokensThisMonth = cached

	return stats, nil
}

// ChannelSuccessRates is the resolver for the channelSuccessRates field.
// Note: Uses request_execution table for channel-level process tracking.
// This provides success/failure rates per channel, suitable for monitoring channel health.
// For result-only channel statistics, use RequestStatsByChannel instead.
func (r *queryResolver) ChannelSuccessRates(ctx context.Context) ([]*ChannelSuccessRate, error) {
	ctx = scopes.WithUserScopeDecision(ctx, scopes.ScopeReadDashboard)

	limitCount := 5

	type channelExecutionStats struct {
		ChannelID    int `json:"channel_id"`
		SuccessCount int `json:"success_count"`
		FailedCount  int `json:"failed_count"`
	}

	var results []channelExecutionStats

	// Use raw SQL to aggregate execution stats by channel
	err := r.client.RequestExecution.Query().
		Modify(func(s *sql.Selector) {
			s.Select(
				requestexecution.FieldChannelID,
				sql.As("SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END)", "success_count"),
				sql.As("SUM(CASE WHEN status = 'failed' THEN 1 ELSE 0 END)", "failed_count"),
			).
				Where(sql.NotNull(requestexecution.FieldChannelID)).
				GroupBy(requestexecution.FieldChannelID)
		}).
		Scan(ctx, &results)
	if err != nil {
		return nil, fmt.Errorf("failed to get channel execution stats: %w", err)
	}

	if len(results) == 0 {
		return []*ChannelSuccessRate{}, nil
	}

	// Build response with success rate calculation
	var response []*ChannelSuccessRate

	for _, result := range results {
		totalCount := result.SuccessCount + result.FailedCount

		var successRate float64
		if totalCount > 0 {
			successRate = float64(result.SuccessCount) / float64(totalCount) * 100
		}

		response = append(response, &ChannelSuccessRate{
			ChannelID:    objects.GUID{Type: "Channel", ID: result.ChannelID},
			ChannelName:  "",
			ChannelType:  "",
			SuccessCount: result.SuccessCount,
			FailedCount:  result.FailedCount,
			TotalCount:   totalCount,
			SuccessRate:  successRate,
		})
	}

	// Order by total count (descending)
	sort.Slice(response, func(i, j int) bool {
		return response[i].TotalCount > response[j].TotalCount
	})

	// Apply limit
	if len(response) > limitCount {
		response = response[:limitCount]
	}

	// Get channel details for the top channels
	channelIDs := lo.Map(response, func(item *ChannelSuccessRate, _ int) int {
		return item.ChannelID.ID
	})

	ctx = schematype.SkipSoftDelete(ctx)

	channels, err := r.client.Channel.Query().
		Where(channel.IDIn(channelIDs...)).
		All(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to get channel details: %w", err)
	}

	channelMap := lo.SliceToMap(channels, func(ch *ent.Channel) (int, *ent.Channel) {
		return ch.ID, ch
	})

	// Fill in channel details
	for _, item := range response {
		if ch, exists := channelMap[item.ChannelID.ID]; exists {
			item.ChannelName = ch.Name
			item.ChannelType = string(ch.Type)
		}
	}

	return response, nil
}
