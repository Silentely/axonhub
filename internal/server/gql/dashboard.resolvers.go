package gql

// This file will be automatically regenerated based on the schema, any resolver implementations
// will be copied through when generating and any unknown code will be moved to the end.
// Code generated by github.com/99designs/gqlgen version v0.17.78-dev

import (
	"context"
	"fmt"
	"time"

	"entgo.io/ent/dialect"
	"entgo.io/ent/dialect/sql"
	"github.com/looplj/axonhub/internal/ent"
	"github.com/looplj/axonhub/internal/ent/channel"
	"github.com/looplj/axonhub/internal/ent/project"
	"github.com/looplj/axonhub/internal/ent/request"
	"github.com/looplj/axonhub/internal/ent/usagelog"
	"github.com/looplj/axonhub/internal/log"
	"github.com/looplj/axonhub/internal/objects"
	"github.com/looplj/axonhub/internal/scopes"
	"github.com/samber/lo"
)

// DashboardOverview is the resolver for the dashboardOverview field.
func (r *queryResolver) DashboardOverview(ctx context.Context) (*DashboardOverview, error) {
	ctx = scopes.WithUserScopeDecision(ctx, scopes.ScopeReadDashboard)

	// Initialize response with defaults to handle partial failures gracefully
	stats := &DashboardOverview{
		TotalUsers:          0,
		TotalRequests:       0,
		FailedRequests:      0,
		AverageResponseTime: nil,
	}

	// Get total counts with defensive error handling
	if totalUsers, err := r.client.User.Query().Count(ctx); err != nil {
		log.Warn(ctx, "failed to count users", log.Cause(err))
	} else {
		stats.TotalUsers = totalUsers
	}

	if totalRequests, err := r.client.Request.Query().Count(ctx); err != nil {
		log.Warn(ctx, "failed to count requests", log.Cause(err))
	} else {
		stats.TotalRequests = totalRequests
	}

	// Get request stats using the dedicated resolver
	if requestStats, err := r.RequestStats(ctx); err != nil {
		log.Warn(ctx, "failed to get request stats", log.Cause(err))
		// Set a default empty RequestStats if there's an error
		stats.RequestStats = &RequestStats{
			RequestsToday:     0,
			RequestsThisWeek:  0,
			RequestsThisMonth: 0,
		}
	} else {
		stats.RequestStats = requestStats
	}

	if failedRequests, err := r.client.Request.Query().
		Where(request.StatusEQ(request.StatusFailed)).
		Count(ctx); err != nil {
		log.Warn(ctx, "failed to count failed requests", log.Cause(err))
	} else {
		stats.FailedRequests = failedRequests
	}

	// TODO: Calculate average response time from request execution data
	// This would require additional database schema changes to store response times

	return stats, nil
}

// RequestStats is the resolver for the requestStats field.
func (r *queryResolver) RequestStats(ctx context.Context) (*RequestStats, error) {
	ctx = scopes.WithUserScopeDecision(ctx, scopes.ScopeReadDashboard)

	// Initialize response with defaults to handle partial failures gracefully
	stats := &RequestStats{
		RequestsToday:     0,
		RequestsThisWeek:  0,
		RequestsThisMonth: 0,
	}

	now := time.Now()
	today := time.Date(now.Year(), now.Month(), now.Day(), 0, 0, 0, 0, now.Location())
	weekAgo := today.AddDate(0, 0, -7)
	monthAgo := today.AddDate(0, -1, 0)

	// Get requests for today
	if requestsToday, err := r.client.Request.Query().
		Where(request.CreatedAtGTE(today)).
		Count(ctx); err != nil {
		log.Warn(ctx, "failed to count today's requests", log.Cause(err))
	} else {
		stats.RequestsToday = requestsToday
	}

	// Get requests for this week
	if requestsThisWeek, err := r.client.Request.Query().
		Where(request.CreatedAtGTE(weekAgo)).
		Count(ctx); err != nil {
		log.Warn(ctx, "failed to count this week's requests", log.Cause(err))
	} else {
		stats.RequestsThisWeek = requestsThisWeek
	}

	// Get requests for this month
	if requestsThisMonth, err := r.client.Request.Query().
		Where(request.CreatedAtGTE(monthAgo)).
		Count(ctx); err != nil {
		log.Warn(ctx, "failed to count this month's requests", log.Cause(err))
	} else {
		stats.RequestsThisMonth = requestsThisMonth
	}

	return stats, nil
}

// RequestStatsByChannel is the resolver for the requestStatsByChannel field.
func (r *queryResolver) RequestStatsByChannel(ctx context.Context) ([]*RequestStatsByChannel, error) {
	ctx = scopes.WithUserScopeDecision(ctx, scopes.ScopeReadDashboard)

	// Use efficient aggregation query to avoid loading all data into memory
	type channelStats struct {
		ChannelID int `json:"channel_id"`
		Count     int `json:"request_count"`
	}

	var results []channelStats

	// Aggregate by channel_id directly in the database using requests table
	err := r.client.Request.Query().
		Where(request.ChannelIDNotNil()). // Only include requests with channel ID set
		GroupBy(request.FieldChannelID).
		Aggregate(ent.As(ent.Count(), "request_count")).
		Scan(ctx, &results)
	if err != nil {
		return nil, fmt.Errorf("failed to get requests by channel: %w", err)
	}

	if len(results) == 0 {
		return []*RequestStatsByChannel{}, nil
	}

	// Get only the channels we need
	channelIDs := lo.Map(results, func(item channelStats, _ int) int {
		return item.ChannelID
	})

	channels, err := r.client.Channel.Query().
		Where(channel.IDIn(channelIDs...)).
		All(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to get channels: %w", err)
	}

	// Create efficient lookup map
	channelMap := lo.SliceToMap(channels, func(ch *ent.Channel) (int, *ent.Channel) {
		return ch.ID, ch
	})

	// Build response efficiently
	var response []*RequestStatsByChannel

	for _, result := range results {
		if ch, exists := channelMap[result.ChannelID]; exists {
			response = append(response, &RequestStatsByChannel{
				ChannelName: ch.Name,
				ChannelType: string(ch.Type),
				Count:       result.Count,
			})
		}
	}

	return response, nil
}

// RequestStatsByModel is the resolver for the requestStatsByModel field.
func (r *queryResolver) RequestStatsByModel(ctx context.Context) ([]*RequestStatsByModel, error) {
	ctx = scopes.WithUserScopeDecision(ctx, scopes.ScopeReadDashboard)

	type modelStats struct {
		ModelID string `json:"model_id"`
		Count   int    `json:"request_count"`
	}

	var results []modelStats

	err := r.client.Request.Query().
		GroupBy(request.FieldModelID).
		Aggregate(ent.As(ent.Count(), "request_count")).
		Scan(ctx, &results)
	if err != nil {
		return nil, fmt.Errorf("failed to get requests by model: %w", err)
	}

	stats := lo.Map(results, func(item modelStats, _ int) *RequestStatsByModel {
		return &RequestStatsByModel{
			ModelID: item.ModelID,
			Count:   item.Count,
		}
	})

	return stats, nil
}

// DailyRequestStats is the resolver for the dailyRequestStats field.
func (r *queryResolver) DailyRequestStats(ctx context.Context, days *int) ([]*DailyRequestStats, error) {
	ctx = scopes.WithUserScopeDecision(ctx, scopes.ScopeReadDashboard)

	daysCount := 30
	if days != nil {
		daysCount = *days
		if daysCount <= 0 || daysCount > 365 {
			return nil, fmt.Errorf("invalid days parameter: must be between 1 and 365")
		}
	}

	now := time.Now()
	startDate := time.Date(now.Year(), now.Month(), now.Day(), 0, 0, 0, 0, now.Location()).AddDate(0, 0, -daysCount+1)

	// Use GROUP BY aggregation for efficient database-level computation
	type dailyStats struct {
		Date  string `json:"date"`
		Count int    `json:"total_count"`
	}

	var results []dailyStats

	// Use raw SQL for complex GROUP BY with conditional counting
	err := r.client.Request.Query().
		Modify(func(s *sql.Selector) {
			// Build a dialect-specific date expression that returns a string 'YYYY-MM-DD'
			var dateExpr string

			switch s.Dialect() {
			case dialect.SQLite:
				// The stored format looks like: "YYYY-MM-DD HH:MM:SS.SSSSSS +0800 CST m=+..."
				// SQLite cannot parse this with strftime; take the leading date directly.
				dateExpr = "substr(created_at, 1, 10)"
			case dialect.MySQL:
				dateExpr = "DATE_FORMAT(created_at, '%Y-%m-%d')"
			case dialect.Postgres:
				dateExpr = "to_char(created_at, 'YYYY-MM-DD')"
			default:
				// Fallback to ANSI-ish cast; many DBs accept this, but not guaranteed
				dateExpr = "DATE(created_at)"
			}

			s.Select(
				sql.As(dateExpr, "date"),
				sql.As(sql.Count("*"), "total_count"),
			).
				GroupBy(dateExpr).
				OrderBy("date")
		}).
		Scan(ctx, &results)
	if err != nil {
		return nil, fmt.Errorf("failed to get daily request stats: %w", err)
	}

	// Create a map for fast lookup of aggregated data
	statsMap := lo.SliceToMap(results, func(item dailyStats) (string, dailyStats) {
		return item.Date, item
	})

	// Build complete response with zero values for missing dates
	response := make([]*DailyRequestStats, 0, daysCount)

	for i := range daysCount {
		date := startDate.AddDate(0, 0, i)
		dateStr := date.Format("2006-01-02")

		if stats, exists := statsMap[dateStr]; exists {
			// Use aggregated data from database
			response = append(response, &DailyRequestStats{
				Date:  dateStr,
				Count: stats.Count,
			})
		} else {
			// Fill missing dates with zero values
			response = append(response, &DailyRequestStats{
				Date:  dateStr,
				Count: 0,
			})
		}
	}

	return response, nil
}

// TopRequestsProjects is the resolver for the topRequestsProjects field.
func (r *queryResolver) TopRequestsProjects(ctx context.Context, limit *int) ([]*TopRequestsProjects, error) {
	ctx = scopes.WithUserScopeDecision(ctx, scopes.ScopeReadDashboard)

	limitCount := 10
	if limit != nil {
		limitCount = *limit
		if limitCount <= 0 || limitCount > 100 {
			return nil, fmt.Errorf("invalid limit parameter: must be between 1 and 100")
		}
	}

	type projectRequestCount struct {
		ProjectID    int `json:"project_id"`
		RequestCount int `json:"request_count"`
	}

	var results []projectRequestCount

	// Use database aggregation without ordering (GroupBy doesn't support Order)
	err := r.client.Request.Query().
		Limit(limitCount).
		Modify(func(s *sql.Selector) {
			s.Select(
				request.FieldProjectID,
				sql.As(sql.Count("*"), "request_count"),
			).
				GroupBy(request.FieldProjectID).
				OrderBy(sql.Desc("request_count"))
		}).
		Scan(ctx, &results)
	if err != nil {
		return nil, fmt.Errorf("failed to get top projects: %w", err)
	}

	if len(results) == 0 {
		return []*TopRequestsProjects{}, nil
	}

	// Get project details for the top projects
	projectIDs := lo.Map(results, func(item projectRequestCount, _ int) int {
		return item.ProjectID
	})

	projects, err := r.client.Project.Query().
		Where(project.IDIn(projectIDs...)).
		All(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to get project details: %w", err)
	}

	projectMap := lo.SliceToMap(projects, func(p *ent.Project) (int, *ent.Project) {
		return p.ID, p
	})

	// Build response with project details
	var response []*TopRequestsProjects

	for _, result := range results {
		if p, exists := projectMap[result.ProjectID]; exists {
			response = append(response, &TopRequestsProjects{
				ProjectID:          objects.GUID{Type: "Project", ID: p.ID},
				ProjectName:        p.Name,
				ProjectDescription: p.Description,
				RequestCount:       result.RequestCount,
			})
		}
	}

	return response, nil
}

// TokenStats is the resolver for the tokenStats field.
func (r *queryResolver) TokenStats(ctx context.Context) (*TokenStats, error) {
	ctx = scopes.WithUserScopeDecision(ctx, scopes.ScopeReadDashboard)

	// Initialize response with defaults to handle partial failures gracefully
	stats := &TokenStats{
		TotalInputTokensToday:      0,
		TotalOutputTokensToday:     0,
		TotalCachedTokensToday:     0,
		TotalInputTokensThisWeek:   0,
		TotalOutputTokensThisWeek:  0,
		TotalCachedTokensThisWeek:  0,
		TotalInputTokensThisMonth:  0,
		TotalOutputTokensThisMonth: 0,
		TotalCachedTokensThisMonth: 0,
	}

	now := time.Now()
	today := time.Date(now.Year(), now.Month(), now.Day(), 0, 0, 0, 0, now.Location())
	weekAgo := today.AddDate(0, 0, -7)
	monthAgo := today.AddDate(0, -1, 0)

	// Helper function to get token sums for a specific time period
	getTokenSums := func(since time.Time) (input, output, cached int) {
		type tokenSums struct {
			InputTokens  int `json:"input_tokens"`
			OutputTokens int `json:"output_tokens"`
			CachedTokens int `json:"cached_tokens"`
		}

		var records []tokenSums

		err := r.client.UsageLog.Query().
			Where(usagelog.CreatedAtGTE(since)).
			Modify(func(s *sql.Selector) {
				s.Select(
					sql.As(sql.Sum(usagelog.FieldPromptTokens), "input_tokens"),
					sql.As(sql.Sum(usagelog.FieldCompletionTokens), "output_tokens"),
					sql.As(sql.Sum(usagelog.FieldPromptCachedTokens), "cached_tokens"),
				)
			}).
			Scan(ctx, &records)
		if err != nil || len(records) == 0 {
			log.Warn(ctx, "failed to aggregate token stats", log.Cause(err))
			return 0, 0, 0
		}

		inputVal := records[0].InputTokens
		outputVal := records[0].OutputTokens
		cachedVal := records[0].CachedTokens

		if log.DebugEnabled(ctx) {
			log.Debug(ctx, "token stats query result",
				log.String("since", since.Format("2006-01-02 15:04:05")),
				log.Int("input", inputVal),
				log.Int("output", outputVal),
				log.Int("cached", cachedVal))
		}

		return inputVal, outputVal, cachedVal
	}

	// Get token stats for today
	input, output, cached := getTokenSums(today)
	stats.TotalInputTokensToday = input
	stats.TotalOutputTokensToday = output
	stats.TotalCachedTokensToday = cached

	// Get token stats for this week
	input, output, cached = getTokenSums(weekAgo)
	stats.TotalInputTokensThisWeek = input
	stats.TotalOutputTokensThisWeek = output
	stats.TotalCachedTokensThisWeek = cached

	// Get token stats for this month
	input, output, cached = getTokenSums(monthAgo)
	stats.TotalInputTokensThisMonth = input
	stats.TotalOutputTokensThisMonth = output
	stats.TotalCachedTokensThisMonth = cached

	return stats, nil
}
