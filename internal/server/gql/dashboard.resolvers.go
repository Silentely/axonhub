package gql

// This file will be automatically regenerated based on the schema, any resolver
// implementations
// will be copied through when generating and any unknown code will be moved to the end.
// Code generated by github.com/99designs/gqlgen

import (
	"context"
	"fmt"
	"sort"
	"time"

	"entgo.io/ent/dialect"
	"entgo.io/ent/dialect/sql"
	"github.com/looplj/axonhub/internal/ent"
	"github.com/looplj/axonhub/internal/ent/apikey"
	"github.com/looplj/axonhub/internal/ent/channel"
	"github.com/looplj/axonhub/internal/ent/project"
	"github.com/looplj/axonhub/internal/ent/request"
	"github.com/looplj/axonhub/internal/ent/requestexecution"
	"github.com/looplj/axonhub/internal/ent/schema/schematype"
	"github.com/looplj/axonhub/internal/ent/usagelog"
	"github.com/looplj/axonhub/internal/log"
	"github.com/looplj/axonhub/internal/objects"
	"github.com/looplj/axonhub/internal/scopes"
	"github.com/samber/lo"
)

// DashboardOverview is the resolver for the dashboardOverview field.
func (r *queryResolver) DashboardOverview(ctx context.Context) (*DashboardOverview, error) {
	ctx = scopes.WithUserScopeDecision(ctx, scopes.ScopeReadDashboard)

	// Initialize response with defaults to handle partial failures gracefully
	stats := &DashboardOverview{
		TotalUsers:          0,
		TotalRequests:       0,
		FailedRequests:      0,
		AverageResponseTime: nil,
	}

	// Get total counts with defensive error handling
	if totalUsers, err := r.client.User.Query().Count(ctx); err != nil {
		log.Warn(ctx, "failed to count users", log.Cause(err))
	} else {
		stats.TotalUsers = totalUsers
	}

	if totalRequests, err := r.client.Request.Query().Count(ctx); err != nil {
		log.Warn(ctx, "failed to count requests", log.Cause(err))
	} else {
		stats.TotalRequests = totalRequests
	}

	// Get request stats using the dedicated resolver
	if requestStats, err := r.RequestStats(ctx); err != nil {
		log.Warn(ctx, "failed to get request stats", log.Cause(err))
		// Set a default empty RequestStats if there's an error
		stats.RequestStats = &RequestStats{
			RequestsToday:     0,
			RequestsThisWeek:  0,
			RequestsLastWeek:  0,
			RequestsThisMonth: 0,
		}
	} else {
		stats.RequestStats = requestStats
	}

	if failedRequests, err := r.client.Request.Query().
		Where(request.StatusEQ(request.StatusFailed)).
		Count(ctx); err != nil {
		log.Warn(ctx, "failed to count failed requests", log.Cause(err))
	} else {
		stats.FailedRequests = failedRequests
	}

	// TODO: Calculate average response time from request execution data
	// This would require additional database schema changes to store response times

	return stats, nil
}

// RequestStats is the resolver for the requestStats field.
func (r *queryResolver) RequestStats(ctx context.Context) (*RequestStats, error) {
	ctx = scopes.WithUserScopeDecision(ctx, scopes.ScopeReadDashboard)

	// Initialize response with defaults to handle partial failures gracefully
	stats := &RequestStats{
		RequestsToday:     0,
		RequestsThisWeek:  0,
		RequestsLastWeek:  0,
		RequestsThisMonth: 0,
	}

	now := time.Now()
	today := time.Date(now.Year(), now.Month(), now.Day(), 0, 0, 0, 0, now.Location())
	weekAgo := today.AddDate(0, 0, -7)
	twoWeeksAgo := today.AddDate(0, 0, -14)
	monthAgo := today.AddDate(0, -1, 0)

	if requestsToday, err := r.client.Request.Query().
		Where(request.CreatedAtGTE(today)).
		Count(ctx); err != nil {
		log.Warn(ctx, "failed to count today's requests", log.Cause(err))
	} else {
		stats.RequestsToday = requestsToday
	}

	if requestsThisWeek, err := r.client.Request.Query().
		Where(request.CreatedAtGTE(weekAgo)).
		Count(ctx); err != nil {
		log.Warn(ctx, "failed to count this week's requests", log.Cause(err))
	} else {
		stats.RequestsThisWeek = requestsThisWeek
	}

	if requestsLastWeek, err := r.client.Request.Query().
		Where(request.CreatedAtGTE(twoWeeksAgo), request.CreatedAtLT(weekAgo)).
		Count(ctx); err != nil {
		log.Warn(ctx, "failed to count last week's requests", log.Cause(err))
	} else {
		stats.RequestsLastWeek = requestsLastWeek
	}

	if requestsThisMonth, err := r.client.Request.Query().
		Where(request.CreatedAtGTE(monthAgo)).
		Count(ctx); err != nil {
		log.Warn(ctx, "failed to count this month's requests", log.Cause(err))
	} else {
		stats.RequestsThisMonth = requestsThisMonth
	}

	return stats, nil
}

// RequestStatsByChannel is the resolver for the requestStatsByChannel field.
func (r *queryResolver) RequestStatsByChannel(ctx context.Context) ([]*RequestStatsByChannel, error) {
	ctx = scopes.WithUserScopeDecision(ctx, scopes.ScopeReadDashboard)

	// Use efficient aggregation query to avoid loading all data into memory
	type channelStats struct {
		ChannelID int `json:"channel_id"`
		Count     int `json:"request_count"`
	}

	var results []channelStats

	// Aggregate by channel_id directly in the database using requests table
	err := r.client.Request.Query().
		Where(request.ChannelIDNotNil()). // Only include requests with channel ID set
		GroupBy(request.FieldChannelID).
		Aggregate(ent.As(ent.Count(), "request_count")).
		Scan(ctx, &results)
	if err != nil {
		return nil, fmt.Errorf("failed to get requests by channel: %w", err)
	}

	if len(results) == 0 {
		return []*RequestStatsByChannel{}, nil
	}

	// Order by request count and keep only top 10
	sort.Slice(results, func(i, j int) bool {
		return results[i].Count > results[j].Count
	})

	if len(results) > 10 {
		results = results[:10]
	}

	// Get only the channels we need
	channelIDs := lo.Map(results, func(item channelStats, _ int) int {
		return item.ChannelID
	})

	channels, err := r.client.Channel.Query().
		Where(channel.IDIn(channelIDs...)).
		All(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to get channels: %w", err)
	}

	// Create efficient lookup map
	channelMap := lo.SliceToMap(channels, func(ch *ent.Channel) (int, *ent.Channel) {
		return ch.ID, ch
	})

	// Build response efficiently
	var response []*RequestStatsByChannel

	for _, result := range results {
		if ch, exists := channelMap[result.ChannelID]; exists {
			response = append(response, &RequestStatsByChannel{
				ChannelName: ch.Name,
				ChannelType: string(ch.Type),
				Count:       result.Count,
			})
		}
	}

	return response, nil
}

// RequestStatsByModel is the resolver for the requestStatsByModel field.
func (r *queryResolver) RequestStatsByModel(ctx context.Context) ([]*RequestStatsByModel, error) {
	ctx = scopes.WithUserScopeDecision(ctx, scopes.ScopeReadDashboard)

	type modelStats struct {
		ModelID string `json:"model_id"`
		Count   int    `json:"request_count"`
	}

	var results []modelStats

	err := r.client.Request.Query().
		GroupBy(request.FieldModelID).
		Aggregate(ent.As(ent.Count(), "request_count")).
		Scan(ctx, &results)
	if err != nil {
		return nil, fmt.Errorf("failed to get requests by model: %w", err)
	}

	// Order by request count and keep only top 10
	sort.Slice(results, func(i, j int) bool {
		return results[i].Count > results[j].Count
	})

	if len(results) > 10 {
		results = results[:10]
	}

	stats := lo.Map(results, func(item modelStats, _ int) *RequestStatsByModel {
		return &RequestStatsByModel{
			ModelID: item.ModelID,
			Count:   item.Count,
		}
	})

	return stats, nil
}

// RequestStatsByAPIKey is the resolver for the requestStatsByAPIKey field.
func (r *queryResolver) RequestStatsByAPIKey(ctx context.Context) ([]*RequestStatsByAPIKey, error) {
	ctx = scopes.WithUserScopeDecision(ctx, scopes.ScopeReadDashboard)

	type apiKeyStats struct {
		APIKeyID int `json:"api_key_id"`
		Count    int `json:"request_count"`
	}

	var results []apiKeyStats

	// Database-level aggregation
	err := r.client.Request.Query().
		Where(request.APIKeyIDNotNil()).
		GroupBy(request.FieldAPIKeyID).
		Aggregate(ent.As(ent.Count(), "request_count")).
		Scan(ctx, &results)
	if err != nil {
		return nil, fmt.Errorf("failed to get requests by API key: %w", err)
	}

	if len(results) == 0 {
		return []*RequestStatsByAPIKey{}, nil
	}

	// Sort by count (descending) and limit to top 10
	sort.Slice(results, func(i, j int) bool {
		return results[i].Count > results[j].Count
	})

	if len(results) > 10 {
		results = results[:10]
	}

	// Extract API key IDs
	apiKeyIDs := lo.Map(results, func(item apiKeyStats, _ int) int {
		return item.APIKeyID
	})

	// Fetch API key details
	apiKeys, err := r.client.APIKey.Query().
		Where(apikey.IDIn(apiKeyIDs...)).
		All(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to get API keys: %w", err)
	}

	// Create lookup map
	apiKeyMap := lo.SliceToMap(apiKeys, func(ak *ent.APIKey) (int, *ent.APIKey) {
		return ak.ID, ak
	})

	// Build response
	var response []*RequestStatsByAPIKey

	for _, result := range results {
		if ak, exists := apiKeyMap[result.APIKeyID]; exists {
			response = append(response, &RequestStatsByAPIKey{
				APIKeyID:   objects.GUID{Type: "APIKey", ID: result.APIKeyID},
				APIKeyName: ak.Name,
				Count:      result.Count,
			})
		}
	}

	return response, nil
}

// TokenStatsByAPIKey is the resolver for the tokenStatsByAPIKey field.
func (r *queryResolver) TokenStatsByAPIKey(ctx context.Context) ([]*TokenStatsByAPIKey, error) {
	ctx = scopes.WithUserScopeDecision(ctx, scopes.ScopeReadDashboard)

	type tokenStats struct {
		APIKeyID        int   `json:"api_key_id"`
		InputTokens     int64 `json:"input_tokens"`
		OutputTokens    int64 `json:"output_tokens"`
		CachedTokens    int64 `json:"cached_tokens"`
		ReasoningTokens int64 `json:"reasoning_tokens"`
	}

	var results []tokenStats

	// Database-level aggregation with JOIN
	err := r.client.UsageLog.Query().
		Modify(func(s *sql.Selector) {
			// Join to requests table to get api_key_id
			requestTable := sql.Table(request.Table)
			s.Join(requestTable).On(
				s.C(usagelog.FieldRequestID),
				requestTable.C(request.FieldID),
			)

			// Filter: only requests with non-null api_key_id
			s.Where(sql.NotNull(requestTable.C(request.FieldAPIKeyID)))

			// Group by api_key_id
			s.GroupBy(requestTable.C(request.FieldAPIKeyID))

			// Select aggregations
			s.Select(
				sql.As(requestTable.C(request.FieldAPIKeyID), "api_key_id"),
				sql.As(sql.Sum(s.C(usagelog.FieldPromptTokens)), "input_tokens"),
				sql.As(sql.Sum(s.C(usagelog.FieldCompletionTokens)), "output_tokens"),
				sql.As(sql.Sum(s.C(usagelog.FieldPromptCachedTokens)), "cached_tokens"),
				sql.As(sql.Sum(s.C(usagelog.FieldCompletionReasoningTokens)), "reasoning_tokens"),
			)
		}).
		Scan(ctx, &results)
	if err != nil {
		return nil, fmt.Errorf("failed to get tokens by API key: %w", err)
	}

	if len(results) == 0 {
		return []*TokenStatsByAPIKey{}, nil
	}

	// Sort by total tokens (descending) and limit to top 3
	sort.Slice(results, func(i, j int) bool {
		totalI := results[i].InputTokens + results[i].OutputTokens +
			results[i].CachedTokens + results[i].ReasoningTokens
		totalJ := results[j].InputTokens + results[j].OutputTokens +
			results[j].CachedTokens + results[j].ReasoningTokens

		return totalI > totalJ
	})

	if len(results) > 3 {
		results = results[:3]
	}

	// Extract API key IDs
	apiKeyIDs := lo.Map(results, func(item tokenStats, _ int) int {
		return item.APIKeyID
	})

	// Fetch API key details
	apiKeys, err := r.client.APIKey.Query().
		Where(apikey.IDIn(apiKeyIDs...)).
		All(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to get API keys: %w", err)
	}

	// Create lookup map
	apiKeyMap := lo.SliceToMap(apiKeys, func(ak *ent.APIKey) (int, *ent.APIKey) {
		return ak.ID, ak
	})

	// Build response
	var response []*TokenStatsByAPIKey

	for _, result := range results {
		if ak, exists := apiKeyMap[result.APIKeyID]; exists {
			totalTokens := result.InputTokens + result.OutputTokens +
				result.CachedTokens + result.ReasoningTokens

			response = append(response, &TokenStatsByAPIKey{
				APIKeyID:        objects.GUID{Type: "APIKey", ID: result.APIKeyID},
				APIKeyName:      ak.Name,
				InputTokens:     int(result.InputTokens),
				OutputTokens:    int(result.OutputTokens),
				CachedTokens:    int(result.CachedTokens),
				ReasoningTokens: int(result.ReasoningTokens),
				TotalTokens:     int(totalTokens),
			})
		}
	}

	return response, nil
}

// DailyRequestStats is the resolver for the dailyRequestStats field.
func (r *queryResolver) DailyRequestStats(ctx context.Context) ([]*DailyRequestStats, error) {
	ctx = scopes.WithUserScopeDecision(ctx, scopes.ScopeReadDashboard)

	daysCount := 30

	now := time.Now()
	startDate := time.Date(now.Year(), now.Month(), now.Day(), 0, 0, 0, 0, now.Location()).AddDate(0, 0, -daysCount+1)

	// Use GROUP BY aggregation for efficient database-level computation
	type dailyStats struct {
		Date   string  `json:"date"`
		Count  int     `json:"total_count"`
		Tokens int     `json:"total_tokens"`
		Cost   float64 `json:"total_cost"`
	}

	var results []dailyStats

	// Use raw SQL for complex GROUP BY with conditional counting
	err := r.client.Request.Query().
		Modify(func(s *sql.Selector) {
			// Build a dialect-specific date expression that returns a string 'YYYY-MM-DD'
			var dateExpr string
			// Use qualified column name to avoid ambiguity when joining
			createdAtCol := s.C(request.FieldCreatedAt)

			switch s.Dialect() {
			case dialect.SQLite:
				// The stored format looks like: "YYYY-MM-DD HH:MM:SS.SSSSSS +0800 CST m=+..."
				// SQLite cannot parse this with strftime; take the leading date directly.
				dateExpr = fmt.Sprintf("substr(%s, 1, 10)", createdAtCol)
			case dialect.MySQL:
				dateExpr = fmt.Sprintf("DATE_FORMAT(%s, '%%Y-%%m-%%d')", createdAtCol)
			case dialect.Postgres:
				dateExpr = fmt.Sprintf("to_char(%s, 'YYYY-MM-DD')", createdAtCol)
			default:
				// Fallback to ANSI-ish cast; many DBs accept this, but not guaranteed
				dateExpr = fmt.Sprintf("DATE(%s)", createdAtCol)
			}

			// Join with usage_logs to get tokens and cost
			usageTable := sql.Table(usagelog.Table)
			s.LeftJoin(usageTable).On(
				s.C(request.FieldID),
				usageTable.C(usagelog.FieldRequestID),
			)

			s.Select(
				sql.As(dateExpr, "date"),
				sql.As(sql.Count(s.C(request.FieldID)), "total_count"),
				sql.As(fmt.Sprintf("COALESCE(SUM(%s), 0)", usageTable.C(usagelog.FieldTotalTokens)), "total_tokens"),
				sql.As(fmt.Sprintf("COALESCE(SUM(%s), 0)", usageTable.C(usagelog.FieldTotalCost)), "total_cost"),
			).
				GroupBy(dateExpr).
				OrderBy("date")
		}).
		Scan(ctx, &results)
	if err != nil {
		return nil, fmt.Errorf("failed to get daily request stats: %w", err)
	}

	// Create a map for fast lookup of aggregated data
	statsMap := lo.SliceToMap(results, func(item dailyStats) (string, dailyStats) {
		return item.Date, item
	})

	// Build complete response with zero values for missing dates
	response := make([]*DailyRequestStats, 0, daysCount)

	for i := range daysCount {
		date := startDate.AddDate(0, 0, i)
		dateStr := date.Format("2006-01-02")

		if stats, exists := statsMap[dateStr]; exists {
			// Use aggregated data from database
			response = append(response, &DailyRequestStats{
				Date:   dateStr,
				Count:  stats.Count,
				Tokens: stats.Tokens,
				Cost:   stats.Cost,
			})
		} else {
			// Fill missing dates with zero values
			response = append(response, &DailyRequestStats{
				Date:   dateStr,
				Count:  0,
				Tokens: 0,
				Cost:   0,
			})
		}
	}

	return response, nil
}

// TopRequestsProjects is the resolver for the topRequestsProjects field.
func (r *queryResolver) TopRequestsProjects(ctx context.Context) ([]*TopRequestsProjects, error) {
	ctx = scopes.WithUserScopeDecision(ctx, scopes.ScopeReadDashboard)

	limitCount := 10

	type projectRequestCount struct {
		ProjectID    int `json:"project_id"`
		RequestCount int `json:"request_count"`
	}

	var results []projectRequestCount

	// Use database aggregation without ordering (GroupBy doesn't support Order)
	err := r.client.Request.Query().
		Limit(limitCount).
		Modify(func(s *sql.Selector) {
			s.Select(
				request.FieldProjectID,
				sql.As(sql.Count("*"), "request_count"),
			).
				GroupBy(request.FieldProjectID).
				OrderBy(sql.Desc("request_count"))
		}).
		Scan(ctx, &results)
	if err != nil {
		return nil, fmt.Errorf("failed to get top projects: %w", err)
	}

	if len(results) == 0 {
		return []*TopRequestsProjects{}, nil
	}

	// Get project details for the top projects
	projectIDs := lo.Map(results, func(item projectRequestCount, _ int) int {
		return item.ProjectID
	})

	projects, err := r.client.Project.Query().
		Where(project.IDIn(projectIDs...)).
		All(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to get project details: %w", err)
	}

	projectMap := lo.SliceToMap(projects, func(p *ent.Project) (int, *ent.Project) {
		return p.ID, p
	})

	// Build response with project details
	var response []*TopRequestsProjects

	for _, result := range results {
		if p, exists := projectMap[result.ProjectID]; exists {
			response = append(response, &TopRequestsProjects{
				ProjectID:          objects.GUID{Type: "Project", ID: p.ID},
				ProjectName:        p.Name,
				ProjectDescription: p.Description,
				RequestCount:       result.RequestCount,
			})
		}
	}

	return response, nil
}

// TokenStats is the resolver for the tokenStats field.
func (r *queryResolver) TokenStats(ctx context.Context) (*TokenStats, error) {
	ctx = scopes.WithUserScopeDecision(ctx, scopes.ScopeReadDashboard)

	// Initialize response with defaults to handle partial failures gracefully
	stats := &TokenStats{
		TotalInputTokensToday:      0,
		TotalOutputTokensToday:     0,
		TotalCachedTokensToday:     0,
		TotalInputTokensThisWeek:   0,
		TotalOutputTokensThisWeek:  0,
		TotalCachedTokensThisWeek:  0,
		TotalInputTokensThisMonth:  0,
		TotalOutputTokensThisMonth: 0,
		TotalCachedTokensThisMonth: 0,
	}

	now := time.Now()
	today := time.Date(now.Year(), now.Month(), now.Day(), 0, 0, 0, 0, now.Location())
	weekAgo := today.AddDate(0, 0, -7)
	monthAgo := today.AddDate(0, -1, 0)

	// Helper function to get token sums for a specific time period
	getTokenSums := func(since time.Time) (input, output, cached int) {
		type tokenSums struct {
			InputTokens  int `json:"input_tokens"`
			OutputTokens int `json:"output_tokens"`
			CachedTokens int `json:"cached_tokens"`
		}

		var records []tokenSums

		err := r.client.UsageLog.Query().
			Where(usagelog.CreatedAtGTE(since)).
			Modify(func(s *sql.Selector) {
				s.Select(
					sql.As(sql.Sum(usagelog.FieldPromptTokens), "input_tokens"),
					sql.As(sql.Sum(usagelog.FieldCompletionTokens), "output_tokens"),
					sql.As(sql.Sum(usagelog.FieldPromptCachedTokens), "cached_tokens"),
				)
			}).
			Scan(ctx, &records)
		if err != nil || len(records) == 0 {
			log.Warn(ctx, "failed to aggregate token stats", log.Cause(err))
			return 0, 0, 0
		}

		inputVal := records[0].InputTokens
		outputVal := records[0].OutputTokens
		cachedVal := records[0].CachedTokens

		if log.DebugEnabled(ctx) {
			log.Debug(ctx, "token stats query result",
				log.String("since", since.Format("2006-01-02 15:04:05")),
				log.Int("input", inputVal),
				log.Int("output", outputVal),
				log.Int("cached", cachedVal))
		}

		return inputVal, outputVal, cachedVal
	}

	// Get token stats for today
	input, output, cached := getTokenSums(today)
	stats.TotalInputTokensToday = input
	stats.TotalOutputTokensToday = output
	stats.TotalCachedTokensToday = cached

	// Get token stats for this week
	input, output, cached = getTokenSums(weekAgo)
	stats.TotalInputTokensThisWeek = input
	stats.TotalOutputTokensThisWeek = output
	stats.TotalCachedTokensThisWeek = cached

	// Get token stats for this month
	input, output, cached = getTokenSums(monthAgo)
	stats.TotalInputTokensThisMonth = input
	stats.TotalOutputTokensThisMonth = output
	stats.TotalCachedTokensThisMonth = cached

	return stats, nil
}

// ChannelSuccessRates is the resolver for the channelSuccessRates field.
func (r *queryResolver) ChannelSuccessRates(ctx context.Context) ([]*ChannelSuccessRate, error) {
	ctx = scopes.WithUserScopeDecision(ctx, scopes.ScopeReadDashboard)

	limitCount := 5

	type channelExecutionStats struct {
		ChannelID    int `json:"channel_id"`
		SuccessCount int `json:"success_count"`
		FailedCount  int `json:"failed_count"`
	}

	var results []channelExecutionStats

	// Use raw SQL to aggregate execution stats by channel
	err := r.client.RequestExecution.Query().
		Modify(func(s *sql.Selector) {
			s.Select(
				requestexecution.FieldChannelID,
				sql.As("SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END)", "success_count"),
				sql.As("SUM(CASE WHEN status = 'failed' THEN 1 ELSE 0 END)", "failed_count"),
			).
				Where(sql.NotNull(requestexecution.FieldChannelID)).
				GroupBy(requestexecution.FieldChannelID)
		}).
		Scan(ctx, &results)
	if err != nil {
		return nil, fmt.Errorf("failed to get channel execution stats: %w", err)
	}

	if len(results) == 0 {
		return []*ChannelSuccessRate{}, nil
	}

	// Build response with success rate calculation
	var response []*ChannelSuccessRate

	for _, result := range results {
		totalCount := result.SuccessCount + result.FailedCount

		var successRate float64
		if totalCount > 0 {
			successRate = float64(result.SuccessCount) / float64(totalCount) * 100
		}

		response = append(response, &ChannelSuccessRate{
			ChannelID:    objects.GUID{Type: "Channel", ID: result.ChannelID},
			ChannelName:  "",
			ChannelType:  "",
			SuccessCount: result.SuccessCount,
			FailedCount:  result.FailedCount,
			TotalCount:   totalCount,
			SuccessRate:  successRate,
		})
	}

	// Order by total count (descending)
	sort.Slice(response, func(i, j int) bool {
		return response[i].TotalCount > response[j].TotalCount
	})

	// Apply limit
	if len(response) > limitCount {
		response = response[:limitCount]
	}

	// Get channel details for the top channels
	channelIDs := lo.Map(response, func(item *ChannelSuccessRate, _ int) int {
		return item.ChannelID.ID
	})

	ctx = schematype.SkipSoftDelete(ctx)

	channels, err := r.client.Channel.Query().
		Where(channel.IDIn(channelIDs...)).
		All(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to get channel details: %w", err)
	}

	channelMap := lo.SliceToMap(channels, func(ch *ent.Channel) (int, *ent.Channel) {
		return ch.ID, ch
	})

	// Fill in channel details
	for _, item := range response {
		if ch, exists := channelMap[item.ChannelID.ID]; exists {
			item.ChannelName = ch.Name
			item.ChannelType = string(ch.Type)
		}
	}

	return response, nil
}
