package gemini

import (
	"strings"

	"github.com/samber/lo"

	"github.com/looplj/axonhub/internal/llm"
)

func extractTextFromContent(content *Content) string {
	if content == nil || len(content.Parts) == 0 {
		return ""
	}

	var texts []string

	for _, part := range content.Parts {
		if part.Text != "" && !part.Thought {
			texts = append(texts, part.Text)
		}
	}

	return strings.Join(texts, "\n")
}

func extractPartsFromLLMMessage(msg *llm.Message) []*Part {
	if msg.Content.Content != nil && *msg.Content.Content != "" {
		return []*Part{{Text: *msg.Content.Content}}
	}

	var parts []*Part

	for _, part := range msg.Content.MultipleContent {
		if part.Type == "text" && part.Text != nil && *part.Text != "" {
			parts = append(parts, &Part{Text: *part.Text})
		}
	}

	return parts
}

func convertGeminiRoleToLLMRole(role string) string {
	switch role {
	case "model":
		return "assistant"
	case "user":
		return "user"
	case "":
		return "user"
	default:
		return role
	}
}

func convertLLMRoleToGeminiRole(role string) string {
	switch role {
	case "assistant":
		return "model"
	case "user":
		return "user"
	default:
		return role
	}
}

// Defines the reason why the model stopped generating tokens.

// Enums
// FINISH_REASON_UNSPECIFIED	Default value. This value is unused.
// STOP	Natural stop point of the model or provided stop sequence.
// MAX_TOKENS	The maximum number of tokens as specified in the request was reached.
// SAFETY	The response candidate content was flagged for safety reasons.
// RECITATION	The response candidate content was flagged for recitation reasons.
// LANGUAGE	The response candidate content was flagged for using an unsupported language.
// OTHER	Unknown reason.
// BLOCKLIST	Token generation stopped because the content contains forbidden terms.
// PROHIBITED_CONTENT	Token generation stopped for potentially containing prohibited content.
// SPII	Token generation stopped because the content potentially contains Sensitive Personally Identifiable Information (SPII).
// MALFORMED_FUNCTION_CALL	The function call generated by the model is invalid.
// IMAGE_SAFETY	Token generation stopped because generated images contain safety violations.
// IMAGE_PROHIBITED_CONTENT	Image generation stopped because generated images has other prohibited content.
// IMAGE_OTHER	Image generation stopped because of other miscellaneous issue.
// NO_IMAGE	The model was expected to generate an image, but none was generated.
// IMAGE_RECITATION	Image generation stopped due to recitation.
// UNEXPECTED_TOOL_CALL	Model generated a tool call but no tools were enabled in the request.
// TOO_MANY_TOOL_CALLS	Model called too many tools consecutively, thus the system exited execution.
// MISSING_THOUGHT_SIGNATURE	Request has at least one thought signature missing.
func convertGeminiFinishReasonToLLM(reason string, hasToolCall bool) *string {
	var llmReason string

	if reason == "" {
		return nil
	}

	switch reason {
	case "STOP":
		llmReason = "stop"
		if hasToolCall {
			llmReason = "tool_calls"
		}
	case "MAX_TOKENS":
		llmReason = "length"
	case "SAFETY":
		llmReason = "content_filter"
	case "RECITATION":
		llmReason = "content_filter"
	default:
		llmReason = "stop"
	}

	return lo.ToPtr(llmReason)
}

func convertLLMFinishReasonToGemini(reason *string) string {
	if reason == nil {
		return ""
	}

	switch *reason {
	case "stop":
		return "STOP"
	case "length":
		return "MAX_TOKENS"
	case "content_filter":
		return "SAFETY"
	case "tool_calls":
		return "STOP"
	default:
		return "STOP"
	}
}

func convertImageURLToGeminiPart(url string) *Part {
	if strings.HasPrefix(url, "data:") {
		// Parse data URL
		parts := strings.SplitN(url, ",", 2)
		if len(parts) == 2 {
			headerParts := strings.Split(parts[0], ";")
			if len(headerParts) >= 1 {
				mimeType := strings.TrimPrefix(headerParts[0], "data:")

				return &Part{
					InlineData: &Blob{
						MIMEType: mimeType,
						Data:     parts[1],
					},
				}
			}
		}
	} else {
		// Regular URL - use FileData
		return &Part{
			FileData: &FileData{
				FileURI: url,
			},
		}
	}

	return nil
}

func convertGeminiFunctionCallingConfigToToolChoice(fcc *FunctionCallingConfig) *llm.ToolChoice {
	if fcc == nil {
		return nil
	}

	tc := &llm.ToolChoice{}

	switch fcc.Mode {
	case "AUTO":
		tc.ToolChoice = lo.ToPtr("auto")
	case "ANY":
		if len(fcc.AllowedFunctionNames) == 1 {
			tc.NamedToolChoice = &llm.NamedToolChoice{
				Type: "function",
				Function: llm.ToolFunction{
					Name: fcc.AllowedFunctionNames[0],
				},
			}
		} else {
			tc.ToolChoice = lo.ToPtr("required")
		}
	case "NONE":
		tc.ToolChoice = lo.ToPtr("none")
	default:
		tc.ToolChoice = lo.ToPtr("auto")
	}

	return tc
}

func convertLLMToolChoiceToGeminiToolConfig(tc *llm.ToolChoice) *ToolConfig {
	if tc == nil {
		return nil
	}

	fcc := &FunctionCallingConfig{}

	// Check if it's a string tool choice
	if tc.ToolChoice != nil {
		switch *tc.ToolChoice {
		case "auto":
			fcc.Mode = "AUTO"
		case "none":
			fcc.Mode = "NONE"
		case "required":
			fcc.Mode = "ANY"
		default:
			fcc.Mode = "AUTO"
		}
	} else if tc.NamedToolChoice != nil {
		// Named tool choice - specific function
		fcc.Mode = "ANY"
		if tc.NamedToolChoice.Function.Name != "" {
			fcc.AllowedFunctionNames = []string{tc.NamedToolChoice.Function.Name}
		}
	} else {
		fcc.Mode = "AUTO"
	}

	return &ToolConfig{FunctionCallingConfig: fcc}
}

func thinkingBudgetToReasoningEffort(budget int64) string {
	switch {
	case budget <= 1024:
		return "low"
	case budget <= 8192:
		return "medium"
	default:
		return "high"
	}
}

func reasoningEffortToThinkingBudget(effort string) int64 {
	switch effort {
	case "low":
		return 1024
	case "medium":
		return 8192
	case "high":
		return 32768
	default:
		return 8192
	}
}

// convertToLLMUsage converts Gemini UsageMetadata to unified Usage format.
func convertToLLMUsage(geminiUsage *UsageMetadata) *llm.Usage {
	if geminiUsage == nil {
		return nil
	}

	usage := &llm.Usage{
		PromptTokens:     geminiUsage.PromptTokenCount,
		CompletionTokens: geminiUsage.CandidatesTokenCount,
		TotalTokens:      geminiUsage.TotalTokenCount,
	}

	// Map cached tokens if present
	if geminiUsage.CachedContentTokenCount > 0 {
		usage.PromptTokensDetails = &llm.PromptTokensDetails{
			CachedTokens: geminiUsage.CachedContentTokenCount,
		}
	}

	// Map thoughts tokens if present
	if geminiUsage.ThoughtsTokenCount > 0 {
		usage.CompletionTokensDetails = &llm.CompletionTokensDetails{
			ReasoningTokens: geminiUsage.ThoughtsTokenCount,
		}
		usage.CompletionTokens += geminiUsage.ThoughtsTokenCount
	}

	if len(geminiUsage.CandidatesTokensDetails) > 0 {
		usage.CompletionModalityTokenDetails = lo.Map(geminiUsage.CandidatesTokensDetails, func(candidate *ModalityTokenCount, _ int) llm.ModalityTokenCount {
			return llm.ModalityTokenCount{
				Modality:   candidate.Modality,
				TokenCount: candidate.TokenCount,
			}
		})
	}

	if len(geminiUsage.PromptTokensDetails) > 0 {
		usage.PromptModalityTokenDetails = lo.Map(geminiUsage.PromptTokensDetails, func(prompt *ModalityTokenCount, _ int) llm.ModalityTokenCount {
			return llm.ModalityTokenCount{
				Modality:   prompt.Modality,
				TokenCount: prompt.TokenCount,
			}
		})
	}

	return usage
}

// convertLLMModalitiesToGemini converts LLM modalities to Gemini responseModalities.
// LLM uses lowercase: "text", "image", "audio"
// Gemini uses uppercase: "TEXT", "IMAGE", "AUDIO".
func convertLLMModalitiesToGemini(modalities []string) []string {
	return lo.Map(modalities, func(m string, _ int) string {
		return strings.ToUpper(m)
	})
}

// convertGeminiModalitiesToLLM converts Gemini responseModalities to LLM modalities.
// Gemini uses uppercase: "TEXT", "IMAGE", "AUDIO"
// LLM uses lowercase: "text", "image", "audio".
func convertGeminiModalitiesToLLM(modalities []string) []string {
	return lo.Map(modalities, func(m string, _ int) string {
		return strings.ToLower(m)
	})
}

func convertToGeminiUsage(chatUsage *llm.Usage) *UsageMetadata {
	if chatUsage == nil {
		return nil
	}

	usage := &UsageMetadata{
		PromptTokenCount:        chatUsage.PromptTokens,
		CandidatesTokenCount:    chatUsage.CompletionTokens,
		TotalTokenCount:         chatUsage.TotalTokens,
		CachedContentTokenCount: 0,
		ThoughtsTokenCount:      0,
		CandidatesTokensDetails: nil,
	}

	if chatUsage.PromptTokensDetails != nil {
		usage.CachedContentTokenCount = chatUsage.PromptTokensDetails.CachedTokens
	}

	if chatUsage.CompletionTokensDetails != nil {
		usage.ThoughtsTokenCount = chatUsage.CompletionTokensDetails.ReasoningTokens
		usage.CandidatesTokenCount = chatUsage.CompletionTokens - usage.ThoughtsTokenCount
	}

	if len(chatUsage.CompletionModalityTokenDetails) > 0 {
		usage.CandidatesTokensDetails = lo.Map(chatUsage.CompletionModalityTokenDetails, func(modalityToken llm.ModalityTokenCount, _ int) *ModalityTokenCount {
			return &ModalityTokenCount{
				Modality:   modalityToken.Modality,
				TokenCount: modalityToken.TokenCount,
			}
		})
	}

	if len(chatUsage.PromptModalityTokenDetails) > 0 {
		usage.PromptTokensDetails = lo.Map(chatUsage.PromptModalityTokenDetails, func(modalityToken llm.ModalityTokenCount, _ int) *ModalityTokenCount {
			return &ModalityTokenCount{
				Modality:   modalityToken.Modality,
				TokenCount: modalityToken.TokenCount,
			}
		})
	}

	return usage
}
