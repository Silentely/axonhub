package gemini

import (
	"strings"

	"github.com/samber/lo"

	"github.com/looplj/axonhub/internal/pkg/xurl"
	"github.com/looplj/axonhub/llm"
)

func extractTextFromContent(content *Content) string {
	if content == nil || len(content.Parts) == 0 {
		return ""
	}

	var texts []string

	for _, part := range content.Parts {
		if part.Text != "" && !part.Thought {
			texts = append(texts, part.Text)
		}
	}

	return strings.Join(texts, "\n")
}

func extractPartsFromLLMMessage(msg *llm.Message) []*Part {
	if msg.Content.Content != nil && *msg.Content.Content != "" {
		return []*Part{{Text: *msg.Content.Content}}
	}

	var parts []*Part

	for _, part := range msg.Content.MultipleContent {
		if part.Type == "text" && part.Text != nil && *part.Text != "" {
			parts = append(parts, &Part{Text: *part.Text})
		}
	}

	return parts
}

func convertGeminiRoleToLLMRole(role string) string {
	switch role {
	case "model":
		return "assistant"
	case "user":
		return "user"
	case "":
		return "user"
	default:
		return role
	}
}

func convertLLMRoleToGeminiRole(role string) string {
	switch role {
	case "assistant":
		return "model"
	case "user":
		return "user"
	case "developer":
		// Developer role is treated as system-equivalent and handled in system instruction
		// Convert to user for any content that needs to be processed as regular content
		return "user"
	default:
		return role
	}
}

// Defines the reason why the model stopped generating tokens.

// Enums
// FINISH_REASON_UNSPECIFIED	Default value. This value is unused.
// STOP	Natural stop point of the model or provided stop sequence.
// MAX_TOKENS	The maximum number of tokens as specified in the request was reached.
// SAFETY	The response candidate content was flagged for safety reasons.
// RECITATION	The response candidate content was flagged for recitation reasons.
// LANGUAGE	The response candidate content was flagged for using an unsupported language.
// OTHER	Unknown reason.
// BLOCKLIST	Token generation stopped because the content contains forbidden terms.
// PROHIBITED_CONTENT	Token generation stopped for potentially containing prohibited content.
// SPII	Token generation stopped because the content potentially contains Sensitive Personally Identifiable Information (SPII).
// MALFORMED_FUNCTION_CALL	The function call generated by the model is invalid.
// IMAGE_SAFETY	Token generation stopped because generated images contain safety violations.
// IMAGE_PROHIBITED_CONTENT	Image generation stopped because generated images has other prohibited content.
// IMAGE_OTHER	Image generation stopped because of other miscellaneous issue.
// NO_IMAGE	The model was expected to generate an image, but none was generated.
// IMAGE_RECITATION	Image generation stopped due to recitation.
// UNEXPECTED_TOOL_CALL	Model generated a tool call but no tools were enabled in the request.
// TOO_MANY_TOOL_CALLS	Model called too many tools consecutively, thus the system exited execution.
// MISSING_THOUGHT_SIGNATURE	Request has at least one thought signature missing.
func convertGeminiFinishReasonToLLM(reason string, hasToolCall bool) *string {
	var llmReason string

	if reason == "" {
		return nil
	}

	switch reason {
	case "STOP":
		llmReason = "stop"
		if hasToolCall {
			llmReason = "tool_calls"
		}
	case "MAX_TOKENS":
		llmReason = "length"
	case "SAFETY":
		llmReason = "content_filter"
	case "RECITATION":
		llmReason = "content_filter"
	default:
		llmReason = "stop"
	}

	return lo.ToPtr(llmReason)
}

func convertLLMFinishReasonToGemini(reason *string) string {
	if reason == nil {
		return ""
	}

	switch *reason {
	case "stop":
		return "STOP"
	case "length":
		return "MAX_TOKENS"
	case "content_filter":
		return "SAFETY"
	case "tool_calls":
		return "STOP"
	default:
		return "STOP"
	}
}

func convertImageURLToGeminiPart(url string) *Part {
	if parsed := xurl.ParseDataURL(url); parsed != nil {
		return &Part{
			InlineData: &Blob{
				MIMEType: parsed.MediaType,
				Data:     parsed.Data,
			},
		}
	}

	// Regular URL - use FileData with MIME type detection
	part := &Part{
		FileData: &FileData{
			FileURI: url,
		},
	}

	return part
}

// convertDocumentURLToGeminiPart converts a DocumentURL to a Gemini Part.
// Handles both data URLs and regular URLs for documents (PDF, Word, etc.)
func convertDocumentURLToGeminiPart(doc *llm.DocumentURL) *Part {
	if doc == nil || doc.URL == "" {
		return nil
	}

	if parsed := xurl.ParseDataURL(doc.URL); parsed != nil {
		// Data URL - use InlineData
		mimeType := parsed.MediaType
		if mimeType == "" && doc.MIMEType != "" {
			mimeType = doc.MIMEType
		}

		return &Part{
			InlineData: &Blob{
				MIMEType: mimeType,
				Data:     parsed.Data,
			},
		}
	}

	// Regular URL - use FileData
	mimeType := doc.MIMEType

	return &Part{
		FileData: &FileData{
			FileURI:  doc.URL,
			MIMEType: mimeType,
		},
	}
}

// isDocumentMIMEType checks if the MIME type represents a document (not an image).
func isDocumentMIMEType(mimeType string) bool {
	if mimeType == "" {
		return false
	}

	mimeType = strings.ToLower(mimeType)

	// Document MIME types
	return strings.HasPrefix(mimeType, "application/pdf") ||
		strings.HasPrefix(mimeType, "application/msword") ||
		strings.HasPrefix(mimeType, "application/vnd.openxmlformats-officedocument") ||
		strings.HasPrefix(mimeType, "application/vnd.ms-") ||
		strings.HasPrefix(mimeType, "text/")
}

func convertGeminiFunctionCallingConfigToToolChoice(fcc *FunctionCallingConfig) *llm.ToolChoice {
	if fcc == nil {
		return nil
	}

	tc := &llm.ToolChoice{}

	switch fcc.Mode {
	case "AUTO":
		tc.ToolChoice = lo.ToPtr("auto")
	case "ANY":
		if len(fcc.AllowedFunctionNames) == 1 {
			tc.NamedToolChoice = &llm.NamedToolChoice{
				Type: "function",
				Function: llm.ToolFunction{
					Name: fcc.AllowedFunctionNames[0],
				},
			}
		} else {
			tc.ToolChoice = lo.ToPtr("required")
		}
	case "NONE":
		tc.ToolChoice = lo.ToPtr("none")
	default:
		tc.ToolChoice = lo.ToPtr("auto")
	}

	return tc
}

func convertLLMToolChoiceToGeminiToolConfig(tc *llm.ToolChoice) *ToolConfig {
	if tc == nil {
		return nil
	}

	fcc := &FunctionCallingConfig{}

	// Check if it's a string tool choice
	if tc.ToolChoice != nil {
		switch *tc.ToolChoice {
		case "auto":
			fcc.Mode = "AUTO"
		case "none":
			fcc.Mode = "NONE"
		case "required":
			fcc.Mode = "ANY"
		default:
			fcc.Mode = "AUTO"
		}
	} else if tc.NamedToolChoice != nil {
		// Named tool choice - specific function
		fcc.Mode = "ANY"
		if tc.NamedToolChoice.Function.Name != "" {
			fcc.AllowedFunctionNames = []string{tc.NamedToolChoice.Function.Name}
		}
	} else {
		fcc.Mode = "AUTO"
	}

	return &ToolConfig{FunctionCallingConfig: fcc}
}

func thinkingBudgetToReasoningEffort(budget int64) string {
	switch {
	case budget <= 1024:
		return "low"
	case budget <= 8192:
		return "medium"
	default:
		return "high"
	}
}

// shouldUseThinkingLevelForBudget returns true if the model is Gemini 3
// and the budget falls within standard effort ranges, allowing use of
// thinkingLevel instead of thinkingBudget. Gemini 3 prefers thinkingLevel.
func shouldUseThinkingLevelForBudget(model string, budget int64) bool {
	if !strings.Contains(strings.ToLower(model), "gemini-3-") {
		return false
	}

	switch {
	case budget <= 1024:
		return true
	case budget <= 8192:
		return true
	case budget >= 1024 && budget <= 32768:
		return true
	default:
		return false
	}
}

// defaultReasoningEffortMapping returns the default mapping from ReasoningEffort to thinking budget tokens.
var defaultGeminiReasoningEffortMapping = map[string]int64{
	"low":    1024,
	"medium": 8192,
	"high":   32768,
}

func reasoningEffortToThinkingBudget(effort string) int64 {
	return reasoningEffortToThinkingBudgetWithConfig(effort, nil)
}

// reasoningEffortToThinkingBudgetWithConfig returns the thinking budget tokens for a given reasoning effort with config.
func reasoningEffortToThinkingBudgetWithConfig(effort string, config *Config) int64 {
	if config != nil && config.ReasoningEffortToBudget != nil {
		if budget, exists := config.ReasoningEffortToBudget[effort]; exists {
			return budget
		}
	}

	// Fall back to default mapping
	if budget, exists := defaultGeminiReasoningEffortMapping[effort]; exists {
		return budget
	}

	// Default to medium if not found
	return 8192
}

// convertToLLMUsage converts Gemini UsageMetadata to unified Usage format.
func convertToLLMUsage(geminiUsage *UsageMetadata) *llm.Usage {
	if geminiUsage == nil {
		return nil
	}

	// IMPORTANT: Token semantics between Gemini and LLM/OpenAI formats
	// - Gemini format: promptTokenCount INCLUDES cachedContentTokenCount (all prompt tokens)
	// - LLM/OpenAI format: prompt_tokens does NOT include cached_tokens (only NEW tokens processed)
	//
	// Example from Gemini response:
	//   promptTokenCount: 20981 (total tokens in prompt)
	//   cachedContentTokenCount: 20350 (tokens served from cache)
	//   Actual new tokens processed: 20981 - 20350 = 631
	//
	// Converted to LLM format should be:
	//   prompt_tokens: 631 (only new tokens)
	//   cached_tokens: 20350 (tokens from cache)
	//
	// This ensures cache hit rate calculation works correctly:
	//   cache_hit_rate = cached_tokens / (prompt_tokens + cached_tokens) * 100
	//   = 20350 / (631 + 20350) * 100 = 97.0%

	promptTokens := geminiUsage.PromptTokenCount
	cachedTokens := geminiUsage.CachedContentTokenCount

	// Subtract cached tokens from prompt tokens to get only NEW tokens
	if cachedTokens > 0 {
		promptTokens = geminiUsage.PromptTokenCount - cachedTokens
	}

	usage := &llm.Usage{
		PromptTokens:     promptTokens,
		CompletionTokens: geminiUsage.CandidatesTokenCount,
		TotalTokens:      geminiUsage.TotalTokenCount,
	}

	// Map cached tokens if present
	if cachedTokens > 0 {
		usage.PromptTokensDetails = &llm.PromptTokensDetails{
			CachedTokens: cachedTokens,
		}
	}

	// Map thoughts tokens if present
	if geminiUsage.ThoughtsTokenCount > 0 {
		usage.CompletionTokensDetails = &llm.CompletionTokensDetails{
			ReasoningTokens: geminiUsage.ThoughtsTokenCount,
		}
		// IMPORTANT: Token semantics in both formats
		// - Gemini format: candidatesTokenCount does NOT include thoughtsTokenCount (they are separate)
		// - LLM/OpenAI format: completion_tokens does NOT include reasoning_tokens (they are separate)
		//
		// Therefore, we directly map CandidatesTokenCount to CompletionTokens without any addition.
		//
		// Example:
		//   Input (Gemini): candidatesTokenCount=64, thoughtsTokenCount=253
		//   Output (LLM):   completion_tokens=64, reasoning_tokens=253
		//
		// The old code incorrectly added: completion_tokens = 64 + 253 = 317 (WRONG!)
		// This would be inconsistent with the reverse conversion (LLM -> Gemini).
	}

	if len(geminiUsage.CandidatesTokensDetails) > 0 {
		usage.CompletionModalityTokenDetails = lo.Map(geminiUsage.CandidatesTokensDetails, func(candidate *ModalityTokenCount, _ int) llm.ModalityTokenCount {
			return llm.ModalityTokenCount{
				Modality:   candidate.Modality,
				TokenCount: candidate.TokenCount,
			}
		})
	}

	if len(geminiUsage.PromptTokensDetails) > 0 {
		usage.PromptModalityTokenDetails = lo.Map(geminiUsage.PromptTokensDetails, func(prompt *ModalityTokenCount, _ int) llm.ModalityTokenCount {
			return llm.ModalityTokenCount{
				Modality:   prompt.Modality,
				TokenCount: prompt.TokenCount,
			}
		})
	}

	return usage
}

// convertLLMModalitiesToGemini converts LLM modalities to Gemini responseModalities.
// LLM uses lowercase: "text", "image", "audio"
// Gemini uses uppercase: "TEXT", "IMAGE", "AUDIO".
func convertLLMModalitiesToGemini(modalities []string) []string {
	return lo.Map(modalities, func(m string, _ int) string {
		return strings.ToUpper(m)
	})
}

// convertGeminiModalitiesToLLM converts Gemini responseModalities to LLM modalities.
// Gemini uses uppercase: "TEXT", "IMAGE", "AUDIO"
// LLM uses lowercase: "text", "image", "audio".
func convertGeminiModalitiesToLLM(modalities []string) []string {
	return lo.Map(modalities, func(m string, _ int) string {
		return strings.ToLower(m)
	})
}

func convertToGeminiUsage(chatUsage *llm.Usage) *UsageMetadata {
	if chatUsage == nil {
		return nil
	}

	// IMPORTANT: Token semantics between LLM/OpenAI and Gemini formats
	// - LLM/OpenAI format: prompt_tokens does NOT include cached_tokens (only NEW tokens processed)
	// - Gemini format: promptTokenCount INCLUDES cachedContentTokenCount (all prompt tokens)
	//
	// Example from LLM format:
	//   prompt_tokens: 631 (only new tokens)
	//   cached_tokens: 20350 (tokens from cache)
	//
	// Converted to Gemini format should be:
	//   promptTokenCount: 21981 (631 + 20350, total tokens in prompt)
	//   cachedContentTokenCount: 20350 (tokens served from cache)
	//
	// This ensures bidirectional conversion consistency.

	cachedTokens := int64(0)
	if chatUsage.PromptTokensDetails != nil {
		cachedTokens = chatUsage.PromptTokensDetails.CachedTokens
	}

	// Add cached tokens to prompt tokens to get TOTAL prompt tokens
	promptTokenCount := chatUsage.PromptTokens + cachedTokens

	usage := &UsageMetadata{
		PromptTokenCount:        promptTokenCount,
		CandidatesTokenCount:    chatUsage.CompletionTokens,
		TotalTokenCount:         chatUsage.TotalTokens,
		CachedContentTokenCount: cachedTokens,
		ThoughtsTokenCount:      0,
		CandidatesTokensDetails: nil,
	}

	if chatUsage.CompletionTokensDetails != nil {
		usage.ThoughtsTokenCount = chatUsage.CompletionTokensDetails.ReasoningTokens
		// IMPORTANT: Token semantics in both formats
		// - LLM/OpenAI format: completion_tokens does NOT include reasoning_tokens (they are separate)
		// - Gemini format: candidatesTokenCount does NOT include thoughtsTokenCount (they are separate)
		//
		// Therefore, we directly map CompletionTokens to CandidatesTokenCount without any subtraction.
		//
		// Example:
		//   Input (LLM):  completion_tokens=64, reasoning_tokens=253
		//   Output (Gemini): candidatesTokenCount=64, thoughtsTokenCount=253
		//
		// The old code incorrectly subtracted: candidatesTokenCount = 64 - 253 = -189 (WRONG!)
		// This caused negative token counts when reasoning tokens exceeded completion tokens.
		usage.CandidatesTokenCount = chatUsage.CompletionTokens
	}

	if len(chatUsage.CompletionModalityTokenDetails) > 0 {
		usage.CandidatesTokensDetails = lo.Map(chatUsage.CompletionModalityTokenDetails, func(modalityToken llm.ModalityTokenCount, _ int) *ModalityTokenCount {
			return &ModalityTokenCount{
				Modality:   modalityToken.Modality,
				TokenCount: modalityToken.TokenCount,
			}
		})
	}

	if len(chatUsage.PromptModalityTokenDetails) > 0 {
		usage.PromptTokensDetails = lo.Map(chatUsage.PromptModalityTokenDetails, func(modalityToken llm.ModalityTokenCount, _ int) *ModalityTokenCount {
			return &ModalityTokenCount{
				Modality:   modalityToken.Modality,
				TokenCount: modalityToken.TokenCount,
			}
		})
	}

	return usage
}
